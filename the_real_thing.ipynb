{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNoDzPSWdmrqWEICPR8KMj+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikxlvii/pytorch/blob/main/the_real_thing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tackling a problem statement from scratch (simple models to neural nets in Pytorch)"
      ],
      "metadata": {
        "id": "W4VIN1yuKrRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"We just got back from a trip to some obscure location, and we brought back a fancy,wall-mounted analog thermometer. It looks great, and it’s a perfect fit for our livingroom. Its only flaw is that it doesn’t show units. Not to worry, we’ve got a plan: we’ll build a dataset of readings and corresponding temperature values in our favorite units, choose a model, adjust its weights iteratively until a measure of the error is low enough, and finally be able to interpret the new readings in units we understand\""
      ],
      "metadata": {
        "id": "DubWrbtnK4hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we'll get the data\n",
        "\n",
        "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
        "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]"
      ],
      "metadata": {
        "id": "lFuhMfAxLCXk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "t_c = torch.tensor(t_c)\n",
        "t_u = torch.tensor(t_u)"
      ],
      "metadata": {
        "id": "r7PpGndALlFH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll use the linear model for this. The two measurements, t_c and t_u might be linearly related to each other. We can write that in the form:\n",
        "\n",
        "t_c = w * t_u + b\n",
        "\n",
        "w and b refer to weights and bias respectively. The weight tells us how much a given input influences the output. The bias is what the output would be if all inputs were zero.\n",
        "\n",
        "We need to estimate these unknown parameters so that the error between the predicted output and the actual output is as low as possible. A loss function is a measure of error which can be used for this purpose. The loss function is high when the error is high and should ideally be as low as possible for a perfect match. Hence, this is an optimization process where we need to find the values of w and b keeping the loss function as low as possible.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1u8zSF8SMmcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss function here, would be the difference between the predicted temperatures and the actual temperature.\n",
        "\n",
        "loss_func = (t_p - t_c)**2 [we need to loss function to be positive]\n",
        "\n"
      ],
      "metadata": {
        "id": "8oeXmtVMPP1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(t_u,w,b):\n",
        "  return w*t_u + b"
      ],
      "metadata": {
        "id": "SRbuhpioPwEH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(t_p,t_c):\n",
        "  squared_diff = (t_p - t_c)**2\n",
        "  return squared_diff.mean()"
      ],
      "metadata": {
        "id": "P0P_U1OGQL0R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.ones(())\n",
        "b = torch.zeros(())\n",
        "\n",
        "t_p = model(t_u,w,b)\n",
        "t_p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2pc3ewsQX5g",
        "outputId": "5cbc59c0-b7da-4ba6-b3d0-0c568028c00b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
              "        48.4000, 60.4000, 68.4000])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_function(t_p,t_c)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMX-bMN1Q2ny",
        "outputId": "857d11dd-0186-4b81-ebb4-4ac749288586"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1763.8848)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " We'll optimize the loss function using the gradient descent algorithm. Gradient descent computes the rate of change of the loss with respect to each parameter, and modify each parameter in the direction of decreasing loss.\n",
        "\n",
        " The main crunch behind Gradient Descent is to change the parameters very slowly along a decreasing loss. The change should be extremely minute (delta = 0.1). A change in w (or b) leads to some change in loss. If the change in loss is negative then we need to increase w (or b) to minimize the loss. If the change in loss is positive then we need to decrease w (or b) to minimize the loss.\n",
        "\n",
        " Now, it is very important to talk about the learning rate. The learning rate will determine the speed at which the parameters will move towards their optimal value. It's like a car, if we go too fast, then we might miss the destination. So it's better to go slow and hence keep the learning rate very small."
      ],
      "metadata": {
        "id": "CDSgvR1EULO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the times, repeated evaluations of the model and loss aren't very fruitful."
      ],
      "metadata": {
        "id": "OyqR7h_qZqD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will analytically take the derivative of the loss with respect to a parameter. This will give us how that parameter and the overall result are connected. If the derivative is positive then when the parameter increases, the loss will increase too. If it's negative then they'll have an inversely proportional relationship with each other.\n",
        "\n",
        "The gradient is referred to as a vector of derivatives.\n"
      ],
      "metadata": {
        "id": "0ITDi4JPtLbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dloss_fn(t_p,t_c):\n",
        "  dsq_diffs = 2 * (t_p - t_c ) / t_p.size(0)\n",
        "  return dsq_diffs"
      ],
      "metadata": {
        "id": "bGjCyLfhYupn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying derivatives to the model\n",
        "\n",
        "def dmodel_dw(t_u,w,b):\n",
        "  return t_u\n",
        "\n",
        "def dmodel_db(t_u,w,b):\n",
        "  return 1.0"
      ],
      "metadata": {
        "id": "BSjlArfuuyv1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_fn(t_u,t_c,t_p,w,b):\n",
        "  dloss_dtp = dloss_fn(t_p, t_c)\n",
        "  dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
        "  dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n",
        "  return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
      ],
      "metadata": {
        "id": "hBIWE0sWvIM9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have everything in place to optimize our parameters. We have to define the number of iterations which we update and optimize the parameter. These training iterations are called Epochs."
      ],
      "metadata": {
        "id": "s-8y0PSowR7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''def training_loop(n_epochs,learning_rate,params,t_u,t_c):\n",
        "  for epoch in range(1,n_epochs+1):\n",
        "    w,b = params\n",
        "    t_p = model(t_u,w,b)\n",
        "    loss = loss_function(t_p,t_c)\n",
        "    grad = grad_fn(t_u,t_c,t_p,w,b)\n",
        "\n",
        "    params = params - learning_rate*grad\n",
        "\n",
        "    print('Epoch %d, Loss %f' % (epoch,float(loss)))\n",
        "  return params'''"
      ],
      "metadata": {
        "id": "FP9F6zipvgrN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "cbec0101-1be5-40ff-e255-211bbe279b22"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"def training_loop(n_epochs,learning_rate,params,t_u,t_c):\\n  for epoch in range(1,n_epochs+1):\\n    w,b = params\\n    t_p = model(t_u,w,b)\\n    loss = loss_function(t_p,t_c)\\n    grad = grad_fn(t_u,t_c,t_p,w,b)\\n\\n    params = params - learning_rate*grad\\n\\n    print('Epoch %d, Loss %f' % (epoch,float(loss)))\\n  return params\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, learning_rate, params, t_u, t_c,\n",
        "                  print_params=True):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        w, b = params\n",
        "\n",
        "        t_p = model(t_u, w, b)  # <1>\n",
        "        loss = loss_function(t_p, t_c)\n",
        "        grad = grad_fn(t_u, t_c, t_p, w, b)  # <2>\n",
        "\n",
        "        params = params - learning_rate * grad\n",
        "\n",
        "        if epoch in {1, 2, 3, 10, 11, 99, 100, 4000, 5000}:  # <3>\n",
        "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "            if print_params:\n",
        "                print('    Params:', params)\n",
        "                print('    Grad:  ', grad)\n",
        "        if epoch in {4, 12, 101}:\n",
        "            print('...')\n",
        "\n",
        "        if not torch.isfinite(loss).all():\n",
        "            break  # <3>\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "9PTelKNizHQ6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop(n_epochs=100,\n",
        "              learning_rate = 1e-2,\n",
        "              params = torch.tensor([1.0,0.0]),\n",
        "              t_u = t_u,\n",
        "              t_c = t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhSWP4-fzTaI",
        "outputId": "f22fe9d5-a20f-40f8-edb4-0fc8add8748e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 1763.884766\n",
            "    Params: tensor([-44.1730,  -0.8260])\n",
            "    Grad:   tensor([4517.2964,   82.6000])\n",
            "Epoch 2, Loss 5802484.500000\n",
            "    Params: tensor([2568.4011,   45.1637])\n",
            "    Grad:   tensor([-261257.4062,   -4598.9702])\n",
            "Epoch 3, Loss 19408029696.000000\n",
            "    Params: tensor([-148527.7344,   -2616.3931])\n",
            "    Grad:   tensor([15109614.0000,   266155.6875])\n",
            "...\n",
            "Epoch 10, Loss 90901105189019073810297959556841472.000000\n",
            "    Params: tensor([3.2144e+17, 5.6621e+15])\n",
            "    Grad:   tensor([-3.2700e+19, -5.7600e+17])\n",
            "Epoch 11, Loss inf\n",
            "    Params: tensor([-1.8590e+19, -3.2746e+17])\n",
            "    Grad:   tensor([1.8912e+21, 3.3313e+19])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.8590e+19, -3.2746e+17])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The losses are becoming inf in the end. That means that the optimization process is unstable. Let us choose a smaller learning rate for this."
      ],
      "metadata": {
        "id": "_smuKgcZ0REi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop(n_epochs = 100,\n",
        "              learning_rate = 1e-4,\n",
        "              params = torch.tensor([1.0,0.0]),\n",
        "              t_u = t_u,\n",
        "              t_c = t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nwlMfVE0pkD",
        "outputId": "894ce30f-d245-49eb-fece-a9a901ac6a63"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 1763.884766\n",
            "    Params: tensor([ 0.5483, -0.0083])\n",
            "    Grad:   tensor([4517.2964,   82.6000])\n",
            "Epoch 2, Loss 323.090515\n",
            "    Params: tensor([ 0.3623, -0.0118])\n",
            "    Grad:   tensor([1859.5493,   35.7843])\n",
            "Epoch 3, Loss 78.929634\n",
            "    Params: tensor([ 0.2858, -0.0135])\n",
            "    Grad:   tensor([765.4666,  16.5122])\n",
            "...\n",
            "Epoch 10, Loss 29.105247\n",
            "    Params: tensor([ 0.2324, -0.0166])\n",
            "    Grad:   tensor([1.4803, 3.0544])\n",
            "Epoch 11, Loss 29.104168\n",
            "    Params: tensor([ 0.2323, -0.0169])\n",
            "    Grad:   tensor([0.5781, 3.0384])\n",
            "...\n",
            "Epoch 99, Loss 29.023582\n",
            "    Params: tensor([ 0.2327, -0.0435])\n",
            "    Grad:   tensor([-0.0533,  3.0226])\n",
            "Epoch 100, Loss 29.022667\n",
            "    Params: tensor([ 0.2327, -0.0438])\n",
            "    Grad:   tensor([-0.0532,  3.0226])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.2327, -0.0438])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's another problem: the updates to the parameters are very small, so the loss decreases very slowly and eventually stalls.\n",
        "\n",
        "We can also normalize the inputs to ensure that the gradients aren't so different. In this case, we can multiply t_u by 0.1.\n",
        "\n",
        "Normalization is usually done to ensure model convergence."
      ],
      "metadata": {
        "id": "FPdUxWpZ1EQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_un = t_u * 0.1"
      ],
      "metadata": {
        "id": "wmT3C0Rv2IdJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop(n_epochs = 5000,\n",
        "              learning_rate = 1e-2,\n",
        "              params=torch.tensor([1.0,0.0]),\n",
        "              t_u = t_un,\n",
        "              t_c = t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvMIZXfp2Z0T",
        "outputId": "e0e5607b-a7e6-45ad-c5a1-01f07d32ba61"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 80.364342\n",
            "    Params: tensor([1.7761, 0.1064])\n",
            "    Grad:   tensor([-77.6140, -10.6400])\n",
            "Epoch 2, Loss 37.574913\n",
            "    Params: tensor([2.0848, 0.1303])\n",
            "    Grad:   tensor([-30.8623,  -2.3864])\n",
            "Epoch 3, Loss 30.871077\n",
            "    Params: tensor([2.2094, 0.1217])\n",
            "    Grad:   tensor([-12.4631,   0.8587])\n",
            "...\n",
            "Epoch 10, Loss 29.030489\n",
            "    Params: tensor([ 2.3232, -0.0710])\n",
            "    Grad:   tensor([-0.5355,  2.9295])\n",
            "Epoch 11, Loss 28.941877\n",
            "    Params: tensor([ 2.3284, -0.1003])\n",
            "    Grad:   tensor([-0.5240,  2.9264])\n",
            "...\n",
            "Epoch 99, Loss 22.214186\n",
            "    Params: tensor([ 2.7508, -2.4910])\n",
            "    Grad:   tensor([-0.4453,  2.5208])\n",
            "Epoch 100, Loss 22.148710\n",
            "    Params: tensor([ 2.7553, -2.5162])\n",
            "    Grad:   tensor([-0.4446,  2.5165])\n",
            "...\n",
            "Epoch 4000, Loss 2.927680\n",
            "    Params: tensor([  5.3643, -17.2853])\n",
            "    Grad:   tensor([-0.0006,  0.0033])\n",
            "Epoch 5000, Loss 2.927648\n",
            "    Params: tensor([  5.3671, -17.3012])\n",
            "    Grad:   tensor([-0.0001,  0.0006])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  5.3671, -17.3012])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us rewrite some of our code to include AutoGrad"
      ],
      "metadata": {
        "id": "9F_rPYb78nBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = torch.tensor([1.0,0.0], requires_grad = True) # That argument is telling PyTorch to track the entire family tree of tensors resulting from operations on params. In other words, any tensor that will have params as an ancestor will have access to the chain of functions that were called to get from params to that tensor."
      ],
      "metadata": {
        "id": "juVlOj-f8rZ1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_function(model(t_u, *params), t_c)\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "xENceLOQ8yeW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8KOoH5n9nht",
        "outputId": "6f149ef2-cfa7-44a2-93bf-6a70a7108f4f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4517.2969,   82.6000])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AutoGrad enabled traning loop\n",
        "\n",
        "def training_loop_grad(n_epochs, learning_rate, params, t_u, t_c):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    if params.grad is not None:\n",
        "      params.grad.zero_()\n",
        "\n",
        "    t_p = model(t_u, *params)\n",
        "    loss = loss_function(t_p, t_c)\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      params -= learning_rate * params.grad\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "      print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "  return params"
      ],
      "metadata": {
        "id": "7YGkdvs99_iV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop_grad(n_epochs=5000,\n",
        "                   learning_rate = 1e-2,\n",
        "                   params=torch.tensor([1.0,0.0], requires_grad=True),\n",
        "                   t_u = t_un,\n",
        "                   t_c = t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RumzJ0y-o3d",
        "outputId": "ce99c78b-b74e-4407-eab5-d8b49759d04e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Loss 7.860115\n",
            "Epoch 1000, Loss 3.828538\n",
            "Epoch 1500, Loss 3.092191\n",
            "Epoch 2000, Loss 2.957698\n",
            "Epoch 2500, Loss 2.933134\n",
            "Epoch 3000, Loss 2.928648\n",
            "Epoch 3500, Loss 2.927830\n",
            "Epoch 4000, Loss 2.927679\n",
            "Epoch 4500, Loss 2.927652\n",
            "Epoch 5000, Loss 2.927647\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  5.3671, -17.3012], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_un"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8N6qNYgbcGF",
        "outputId": "8cffa287-1619-473e-f6ef-381e57a4f544"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.5700, 5.5900, 5.8200, 8.1900, 5.6300, 4.8900, 3.3900, 2.1800, 4.8400,\n",
              "        6.0400, 6.8400])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values of the parameters come down to 5.3671 and -17.3012 which is really close to the actual values of conversion between Celcius and Fahrenheit.\n"
      ],
      "metadata": {
        "id": "OjoDSwxUH5_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We were actually look at temperature in Fahrenheit all along."
      ],
      "metadata": {
        "id": "kGRrNhVtIFyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic building block of a neural network is a neuron. It's a linear transformation of the input followed by the application of a fixed non linear function (activation function).\n",
        "\n",
        "Mathematically, we can write this out as o = f(w * x + b), with x as our input, w our weight or scaling factor, and b as our bias or offset. f is our activation function.\n",
        "\n",
        "So basically an artificial neuron is a linear transformation enclosed in a non-linear one. Activation functions are extremely important and are the point of difference between a linear model and an artificial neural network.\n",
        "\n",
        "There are two advantages of using them:\n",
        "1. In the inner parts of the model, it allows the output function to have different\n",
        "slopes at different values—something a linear function by definition cannot do.\n",
        "By trickily composing these differently sloped parts for many outputs, neural\n",
        "networks can approximate arbitrary functions, as we will see in section 6.1.6.2\n",
        "2. At the last layer of the network, it has the role of concentrating the outputs of the preceding linear operation into a given range.\n",
        "\n",
        "\n",
        "Another family of functions that work well is torch.nn.Sigmoid, which includes 1 /\n",
        "(1 + e ** -x), torch.tanh, and others that we’ll see in a moment. These functions\n",
        "have a curve that asymptotically approaches 0 or –1 as x goes to negative infinity,\n",
        "approaches 1 as x increases, and have a mostly constant slope at x == 0. Conceptually,\n",
        "functions shaped this way work well because there’s an area in the middle of our linear function’s output that our neuron (which, again, is just a linear function followed\n",
        "by an activation) will be sensitive to, while everything else gets lumped next to the\n",
        "boundary values."
      ],
      "metadata": {
        "id": "v1_sLUzWZ1Ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "x = torch.ones(10,1)\n",
        "linear_model = nn.Linear(1,1)\n",
        "linear_model(x)"
      ],
      "metadata": {
        "id": "jJKv8jefIJ8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cac619a-9385-4263-ab62-fedb7a00790b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4863],\n",
              "        [0.4863],\n",
              "        [0.4863],\n",
              "        [0.4863],\n",
              "        [0.4863],\n",
              "        [0.4863],\n",
              "        [0.4863],\n",
              "        [0.4863],\n",
              "        [0.4863],\n",
              "        [0.4863]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any module in nn is written to produce outputs for a batch of multiple inputs at the same time."
      ],
      "metadata": {
        "id": "exnh3qpYcvFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
        "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
        "\n",
        "t_c = torch.tensor(t_c).unsqueeze(1)\n",
        "t_u = torch.tensor(t_u).unsqueeze(1)"
      ],
      "metadata": {
        "id": "G01QdhfTcVyV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_u.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlDwebgfeBnu",
        "outputId": "1a42beee-daaa-460d-e887-0332f815a33d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([11, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model = nn.Linear(1,1)\n",
        "optimizer = torch.optim.SGD(linear_model.parameters(),lr=1e-2)"
      ],
      "metadata": {
        "id": "lloYWKkteCcd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7foteECEeWbt",
        "outputId": "5fd5f916-f164-42b9-8b75-104134ee5387"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7c8a3f0811c0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(linear_model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJfeg4SDeb_t",
        "outputId": "f383bfbc-b45d-4608-cc2f-fdf7b44a99d8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.3398]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.2062], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to split the dataset into training, validation and testing sets.\n",
        "\n",
        "n_samples = t_u.shape[0]\n",
        "n_val = int(0.2*n_samples)\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]"
      ],
      "metadata": {
        "id": "2Vk-D-ndejnd"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_indices, val_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5vKlH-ohpYO",
        "outputId": "c9f63c4f-7229-4ef5-f5fb-37d2fb0186f4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([10,  0,  1,  3,  5,  2,  4,  9,  8]), tensor([7, 6]))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_t_u = t_u[train_indices]\n",
        "train_t_c = t_c[train_indices]\n",
        "val_t_u = t_u[val_indices]\n",
        "val_t_c = t_c[val_indices]\n",
        "train_t_un = 0.1 * train_t_u\n",
        "val_t_un = 0.1 * val_t_u"
      ],
      "metadata": {
        "id": "5fHath5XhrxT"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the training loop\n",
        "\n",
        "def training_loop_net(n_epochs,optimizer,model,loss_fn,t_u_train,t_u_val,t_c_train,t_c_val):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    t_p_train = model(t_u_train)\n",
        "    loss_train = loss_fn(t_p_train,t_c_train)\n",
        "\n",
        "    t_p_val = model(t_u_val)\n",
        "    loss_val = loss_fn(t_p_val,t_c_val)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch == 1 or epoch % 1000 == 0:\n",
        "      print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\"f\" Validation loss {loss_val.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "NzDMbC6Kh77m"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declaring the training loop\n",
        "\n",
        "linear_model = nn.Linear(1,1)\n",
        "optimizer = torch.optim.SGD(linear_model.parameters(),lr = 1e-2)\n",
        "\n",
        "training_loop_net(n_epochs = 3000,\n",
        "                  optimizer = optimizer,\n",
        "                  model = linear_model,\n",
        "                  loss_fn = nn.MSELoss(),\n",
        "                  t_u_train = train_t_un,\n",
        "                  t_u_val = val_t_un,\n",
        "                  t_c_train = train_t_c,\n",
        "                  t_c_val = val_t_c)\n",
        "print()\n",
        "print(linear_model.weight)\n",
        "print(linear_model.bias)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnaGe83XjM0e",
        "outputId": "dc11213c-5259-4902-97e3-7541454ff9bf"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 326.1721, Validation loss 15.5448\n",
            "Epoch 1000, Training loss 6.0728, Validation loss 1.1863\n",
            "Epoch 2000, Training loss 2.4964, Validation loss 5.3499\n",
            "Epoch 3000, Training loss 1.8561, Validation loss 12.9346\n",
            "\n",
            "Parameter containing:\n",
            "tensor([[5.8478]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-20.3833], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fttqd1E6lWgU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}