{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOQsAWo9LgqX2lQJmlggHjX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikxlvii/pytorch/blob/main/the_real_thing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tackling a problem statement from scratch (simple models to neural nets in Pytorch)"
      ],
      "metadata": {
        "id": "W4VIN1yuKrRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"We just got back from a trip to some obscure location, and we brought back a fancy,wall-mounted analog thermometer. It looks great, and it’s a perfect fit for our livingroom. Its only flaw is that it doesn’t show units. Not to worry, we’ve got a plan: we’ll build a dataset of readings and corresponding temperature values in our favorite units, choose a model, adjust its weights iteratively until a measure of the error is low enough, and finally be able to interpret the new readings in units we understand\""
      ],
      "metadata": {
        "id": "DubWrbtnK4hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we'll get the data\n",
        "\n",
        "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
        "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]"
      ],
      "metadata": {
        "id": "lFuhMfAxLCXk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "t_c = torch.tensor(t_c)\n",
        "t_u = torch.tensor(t_u)"
      ],
      "metadata": {
        "id": "r7PpGndALlFH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll use the linear model for this. The two measurements, t_c and t_u might be linearly related to each other. We can write that in the form:\n",
        "\n",
        "t_c = w * t_u + b\n",
        "\n",
        "w and b refer to weights and bias respectively. The weight tells us how much a given input influences the output. The bias is what the output would be if all inputs were zero.\n",
        "\n",
        "We need to estimate these unknown parameters so that the error between the predicted output and the actual output is as low as possible. A loss function is a measure of error which can be used for this purpose. The loss function is high when the error is high and should ideally be as low as possible for a perfect match. Hence, this is an optimization process where we need to find the values of w and b keeping the loss function as low as possible.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1u8zSF8SMmcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss function here, would be the difference between the predicted temperatures and the actual temperature.\n",
        "\n",
        "loss_func = (t_p - t_c)**2 [we need to loss function to be positive]\n",
        "\n"
      ],
      "metadata": {
        "id": "8oeXmtVMPP1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(t_u,w,b):\n",
        "  return w*t_u + b"
      ],
      "metadata": {
        "id": "SRbuhpioPwEH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(t_p,t_c):\n",
        "  squared_diff = (t_p - t_c)**2\n",
        "  return squared_diff.mean()"
      ],
      "metadata": {
        "id": "P0P_U1OGQL0R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.ones(())\n",
        "b = torch.zeros(())\n",
        "\n",
        "t_p = model(t_u,w,b)\n",
        "t_p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2pc3ewsQX5g",
        "outputId": "238931a6-acde-41a2-ab61-500728dc6aae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
              "        48.4000, 60.4000, 68.4000])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_function(t_p,t_c)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMX-bMN1Q2ny",
        "outputId": "59b65413-ec44-4f67-8376-a28f60c79cb0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1763.8848)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " We'll optimize the loss function using the gradient descent algorithm. Gradient descent computes the rate of change of the loss with respect to each parameter, and modify each parameter in the direction of decreasing loss.\n",
        "\n",
        " The main crunch behind Gradient Descent is to change the parameters very slowly along a decreasing loss. The change should be extremely minute (delta = 0.1). A change in w (or b) leads to some change in loss. If the change in loss is negative then we need to increase w (or b) to minimize the loss. If the change in loss is positive then we need to decrease w (or b) to minimize the loss.\n",
        "\n",
        " Now, it is very important to talk about the learning rate. The learning rate will determine the speed at which the parameters will move towards their optimal value. It's like a car, if we go too fast, then we might miss the destination. So it's better to go slow and hence keep the learning rate very small."
      ],
      "metadata": {
        "id": "CDSgvR1EULO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the times, repeated evaluations of the model and loss aren't very fruitful."
      ],
      "metadata": {
        "id": "OyqR7h_qZqD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will analytically take the derivative of the loss with respect to a parameter. This will give us how that parameter and the overall result are connected. If the derivative is positive then when the parameter increases, the loss will increase too. If it's negative then they'll have an inversely proportional relationship with each other.\n",
        "\n",
        "The gradient is referred to as a vector of derivatives.\n"
      ],
      "metadata": {
        "id": "0ITDi4JPtLbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dloss_fn(t_p,t_c):\n",
        "  dsq_diffs = 2 * (t_p - t_c ) / t_p.size(0)\n",
        "  return dsq_diffs"
      ],
      "metadata": {
        "id": "bGjCyLfhYupn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying derivatives to the model\n",
        "\n",
        "def dmodel_dw(t_u,w,b):\n",
        "  return t_u\n",
        "\n",
        "def dmodel_db(t_u,w,b):\n",
        "  return 1.0"
      ],
      "metadata": {
        "id": "BSjlArfuuyv1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_fn(t_u,t_c,t_p,w,b):\n",
        "  dloss_dtp = dloss_fn(t_p, t_c)\n",
        "  dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
        "  dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n",
        "  return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
      ],
      "metadata": {
        "id": "hBIWE0sWvIM9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have everything in place to optimize our parameters. We have to define the number of iterations which we update and optimize the parameter. These training iterations are called Epochs."
      ],
      "metadata": {
        "id": "s-8y0PSowR7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''def training_loop(n_epochs,learning_rate,params,t_u,t_c):\n",
        "  for epoch in range(1,n_epochs+1):\n",
        "    w,b = params\n",
        "    t_p = model(t_u,w,b)\n",
        "    loss = loss_function(t_p,t_c)\n",
        "    grad = grad_fn(t_u,t_c,t_p,w,b)\n",
        "\n",
        "    params = params - learning_rate*grad\n",
        "\n",
        "    print('Epoch %d, Loss %f' % (epoch,float(loss)))\n",
        "  return params'''"
      ],
      "metadata": {
        "id": "FP9F6zipvgrN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, learning_rate, params, t_u, t_c,\n",
        "                  print_params=True):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        w, b = params\n",
        "\n",
        "        t_p = model(t_u, w, b)  # <1>\n",
        "        loss = loss_function(t_p, t_c)\n",
        "        grad = grad_fn(t_u, t_c, t_p, w, b)  # <2>\n",
        "\n",
        "        params = params - learning_rate * grad\n",
        "\n",
        "        if epoch in {1, 2, 3, 10, 11, 99, 100, 4000, 5000}:  # <3>\n",
        "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "            if print_params:\n",
        "                print('    Params:', params)\n",
        "                print('    Grad:  ', grad)\n",
        "        if epoch in {4, 12, 101}:\n",
        "            print('...')\n",
        "\n",
        "        if not torch.isfinite(loss).all():\n",
        "            break  # <3>\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "9PTelKNizHQ6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop(n_epochs=100,\n",
        "              learning_rate = 1e-2,\n",
        "              params = torch.tensor([1.0,0.0]),\n",
        "              t_u = t_u,\n",
        "              t_c = t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhSWP4-fzTaI",
        "outputId": "01ff5a98-1130-486a-dde5-95c2a97c2d2a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 1763.884766\n",
            "    Params: tensor([-44.1730,  -0.8260])\n",
            "    Grad:   tensor([4517.2964,   82.6000])\n",
            "Epoch 2, Loss 5802484.500000\n",
            "    Params: tensor([2568.4011,   45.1637])\n",
            "    Grad:   tensor([-261257.4062,   -4598.9702])\n",
            "Epoch 3, Loss 19408029696.000000\n",
            "    Params: tensor([-148527.7344,   -2616.3931])\n",
            "    Grad:   tensor([15109614.0000,   266155.6875])\n",
            "...\n",
            "Epoch 10, Loss 90901105189019073810297959556841472.000000\n",
            "    Params: tensor([3.2144e+17, 5.6621e+15])\n",
            "    Grad:   tensor([-3.2700e+19, -5.7600e+17])\n",
            "Epoch 11, Loss inf\n",
            "    Params: tensor([-1.8590e+19, -3.2746e+17])\n",
            "    Grad:   tensor([1.8912e+21, 3.3313e+19])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.8590e+19, -3.2746e+17])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The losses are becoming inf in the end. That means that the optimization process is unstable. Let us choose a smaller learning rate for this."
      ],
      "metadata": {
        "id": "_smuKgcZ0REi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop(n_epochs = 100,\n",
        "              learning_rate = 1e-4,\n",
        "              params = torch.tensor([1.0,0.0]),\n",
        "              t_u = t_u,\n",
        "              t_c = t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nwlMfVE0pkD",
        "outputId": "40713625-89df-4fa5-c109-357c5e882c32"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 1763.884766\n",
            "    Params: tensor([ 0.5483, -0.0083])\n",
            "    Grad:   tensor([4517.2964,   82.6000])\n",
            "Epoch 2, Loss 323.090515\n",
            "    Params: tensor([ 0.3623, -0.0118])\n",
            "    Grad:   tensor([1859.5493,   35.7843])\n",
            "Epoch 3, Loss 78.929634\n",
            "    Params: tensor([ 0.2858, -0.0135])\n",
            "    Grad:   tensor([765.4666,  16.5122])\n",
            "...\n",
            "Epoch 10, Loss 29.105247\n",
            "    Params: tensor([ 0.2324, -0.0166])\n",
            "    Grad:   tensor([1.4803, 3.0544])\n",
            "Epoch 11, Loss 29.104168\n",
            "    Params: tensor([ 0.2323, -0.0169])\n",
            "    Grad:   tensor([0.5781, 3.0384])\n",
            "...\n",
            "Epoch 99, Loss 29.023582\n",
            "    Params: tensor([ 0.2327, -0.0435])\n",
            "    Grad:   tensor([-0.0533,  3.0226])\n",
            "Epoch 100, Loss 29.022667\n",
            "    Params: tensor([ 0.2327, -0.0438])\n",
            "    Grad:   tensor([-0.0532,  3.0226])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.2327, -0.0438])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's another problem: the updates to the parameters are very small, so the loss decreases very slowly and eventually stalls.\n",
        "\n",
        "We can also normalize the inputs to ensure that the gradients aren't so different. In this case, we can multiply t_u by 0.1.\n",
        "\n",
        "Normalization is usually done to ensure model convergence."
      ],
      "metadata": {
        "id": "FPdUxWpZ1EQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_un = t_u * 0.1"
      ],
      "metadata": {
        "id": "wmT3C0Rv2IdJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop(n_epochs = 5000,\n",
        "              learning_rate = 1e-2,\n",
        "              params=torch.tensor([1.0,0.0]),\n",
        "              t_u = t_un,\n",
        "              t_c = t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvMIZXfp2Z0T",
        "outputId": "1ce98026-741d-4293-801a-bf50d4adb3e5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 80.364342\n",
            "    Params: tensor([1.7761, 0.1064])\n",
            "    Grad:   tensor([-77.6140, -10.6400])\n",
            "Epoch 2, Loss 37.574913\n",
            "    Params: tensor([2.0848, 0.1303])\n",
            "    Grad:   tensor([-30.8623,  -2.3864])\n",
            "Epoch 3, Loss 30.871077\n",
            "    Params: tensor([2.2094, 0.1217])\n",
            "    Grad:   tensor([-12.4631,   0.8587])\n",
            "...\n",
            "Epoch 10, Loss 29.030489\n",
            "    Params: tensor([ 2.3232, -0.0710])\n",
            "    Grad:   tensor([-0.5355,  2.9295])\n",
            "Epoch 11, Loss 28.941877\n",
            "    Params: tensor([ 2.3284, -0.1003])\n",
            "    Grad:   tensor([-0.5240,  2.9264])\n",
            "...\n",
            "Epoch 99, Loss 22.214186\n",
            "    Params: tensor([ 2.7508, -2.4910])\n",
            "    Grad:   tensor([-0.4453,  2.5208])\n",
            "Epoch 100, Loss 22.148710\n",
            "    Params: tensor([ 2.7553, -2.5162])\n",
            "    Grad:   tensor([-0.4446,  2.5165])\n",
            "...\n",
            "Epoch 4000, Loss 2.927680\n",
            "    Params: tensor([  5.3643, -17.2853])\n",
            "    Grad:   tensor([-0.0006,  0.0033])\n",
            "Epoch 5000, Loss 2.927648\n",
            "    Params: tensor([  5.3671, -17.3012])\n",
            "    Grad:   tensor([-0.0001,  0.0006])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  5.3671, -17.3012])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us rewrite some of our code to include AutoGrad"
      ],
      "metadata": {
        "id": "9F_rPYb78nBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = torch.tensor([1.0,0.0], requires_grad = True) # That argument is telling PyTorch to track the entire family tree of tensors resulting from operations on params. In other words, any tensor that will have params as an ancestor will have access to the chain of functions that were called to get from params to that tensor."
      ],
      "metadata": {
        "id": "juVlOj-f8rZ1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_function(model(t_u, *params), t_c)\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "xENceLOQ8yeW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8KOoH5n9nht",
        "outputId": "01525e16-4c05-4967-d6dc-2e6861378bfe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4517.2969,   82.6000])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AutoGrad enabled traning loop\n",
        "\n",
        "def training_loop_grad(n_epochs, learning_rate, params, t_u, t_c):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    if params.grad is not None:\n",
        "      params.grad.zero_()\n",
        "\n",
        "    t_p = model(t_u, *params)\n",
        "    loss = loss_function(t_p, t_c)\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      params -= learning_rate * params.grad\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "      print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "  return params"
      ],
      "metadata": {
        "id": "7YGkdvs99_iV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop_grad(n_epochs=5000,\n",
        "                   learning_rate = 1e-2,\n",
        "                   params=torch.tensor([1.0,0.0], requires_grad=True),\n",
        "                   t_u = t_un,\n",
        "                   t_c = t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RumzJ0y-o3d",
        "outputId": "0d9ca52c-37e8-47dd-b05f-04edec393085"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Loss 7.860115\n",
            "Epoch 1000, Loss 3.828538\n",
            "Epoch 1500, Loss 3.092191\n",
            "Epoch 2000, Loss 2.957698\n",
            "Epoch 2500, Loss 2.933134\n",
            "Epoch 3000, Loss 2.928648\n",
            "Epoch 3500, Loss 2.927830\n",
            "Epoch 4000, Loss 2.927679\n",
            "Epoch 4500, Loss 2.927652\n",
            "Epoch 5000, Loss 2.927647\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  5.3671, -17.3012], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values of the parameters come down to 5.3671 and -17.3012 which is really close to the actual values of conversion between Celcius and Fahrenheit.\n"
      ],
      "metadata": {
        "id": "OjoDSwxUH5_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We were actually look at temperature in Fahrenheit all along."
      ],
      "metadata": {
        "id": "kGRrNhVtIFyt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jJKv8jefIJ8G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}