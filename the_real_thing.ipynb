{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyORepu6ESgp7eKazHOHT+Id",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikxlvii/pytorch/blob/main/the_real_thing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tackling a problem statement from scratch (simple models to neural nets in Pytorch)"
      ],
      "metadata": {
        "id": "W4VIN1yuKrRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"We just got back from a trip to some obscure location, and we brought back a fancy,wall-mounted analog thermometer. It looks great, and it’s a perfect fit for our livingroom. Its only flaw is that it doesn’t show units. Not to worry, we’ve got a plan: we’ll build a dataset of readings and corresponding temperature values in our favorite units, choose a model, adjust its weights iteratively until a measure of the error is low enough, and finally be able to interpret the new readings in units we understand\""
      ],
      "metadata": {
        "id": "DubWrbtnK4hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we'll get the data\n",
        "\n",
        "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
        "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]"
      ],
      "metadata": {
        "id": "lFuhMfAxLCXk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "t_c = torch.tensor(t_c)\n",
        "t_u = torch.tensor(t_u)"
      ],
      "metadata": {
        "id": "r7PpGndALlFH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll use the linear model for this. The two measurements, t_c and t_u might be linearly related to each other. We can write that in the form:\n",
        "\n",
        "t_c = w * t_u + b\n",
        "\n",
        "w and b refer to weights and bias respectively. The weight tells us how much a given input influences the output. The bias is what the output would be if all inputs were zero.\n",
        "\n",
        "We need to estimate these unknown parameters so that the error between the predicted output and the actual output is as low as possible. A loss function is a measure of error which can be used for this purpose. The loss function is high when the error is high and should ideally be as low as possible for a perfect match. Hence, this is an optimization process where we need to find the values of w and b keeping the loss function as low as possible.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1u8zSF8SMmcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss function here, would be the difference between the predicted temperatures and the actual temperature.\n",
        "\n",
        "loss_func = (t_p - t_c)**2 [we need to loss function to be positive]\n",
        "\n"
      ],
      "metadata": {
        "id": "8oeXmtVMPP1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(t_u,w,b):\n",
        "  return w*t_u + b"
      ],
      "metadata": {
        "id": "SRbuhpioPwEH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(t_p,t_c):\n",
        "  squared_diff = (t_p - t_c)**2\n",
        "  return squared_diff.mean()"
      ],
      "metadata": {
        "id": "P0P_U1OGQL0R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.ones(())\n",
        "b = torch.zeros(())\n",
        "\n",
        "t_p = model(t_u,w,b)\n",
        "t_p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2pc3ewsQX5g",
        "outputId": "e5f860f4-0d21-411b-eba4-b8fb2143c2f1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
              "        48.4000, 60.4000, 68.4000])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_function(t_p,t_c)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMX-bMN1Q2ny",
        "outputId": "44256dd5-e09d-4647-c2c0-756404db29c2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1763.8848)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " We'll optimize the loss function using the gradient descent algorithm. Gradient descent computes the rate of change of the loss with respect to each parameter, and modify each parameter in the direction of decreasing loss.\n",
        "\n",
        " The main crunch behind Gradient Descent is to change the parameters very slowly along a decreasing loss. The change should be extremely minute (delta = 0.1). A change in w (or b) leads to some change in loss. If the change in loss is negative then we need to increase w (or b) to minimize the loss. If the change in loss is positive then we need to decrease w (or b) to minimize the loss.\n",
        "\n",
        " Now, it is very important to talk about the learning rate. The learning rate will determine the speed at which the parameters will move towards their optimal value. It's like a car, if we go too fast, then we might miss the destination. So it's better to go slow and hence keep the learning rate very small."
      ],
      "metadata": {
        "id": "CDSgvR1EULO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the times, repeated evaluations of the model and loss aren't very fruitful."
      ],
      "metadata": {
        "id": "OyqR7h_qZqD3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bGjCyLfhYupn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}